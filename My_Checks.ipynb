{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This is for my own setup\n",
        "\n",
        "!ls -la /content\n",
        "!git clone https://github.com/mehrdad-mirpourian/nlu_hw2.git\n",
        "!ls -la /content/nlu_hw2\n",
        "\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/nlu_hw2\")\n",
        "print(\"Current directory:\", os.getcwd())  # Should print /content/nlu_hw2\n",
        "\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC1KUaCxjRTH",
        "outputId": "b7727a10-1014-4990-a8b4-309329248d9f"
      },
      "id": "GC1KUaCxjRTH",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Mar 10 00:08 .\n",
            "drwxr-xr-x 1 root root 4096 Mar 10 00:03 ..\n",
            "drwxr-xr-x 4 root root 4096 Mar  6 14:29 .config\n",
            "drwxr-xr-x 3 root root 4096 Mar 10 00:08 nlu_hw2\n",
            "drwxr-xr-x 1 root root 4096 Mar  6 14:29 sample_data\n",
            "Cloning into 'nlu_hw2'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 45 (delta 24), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (45/45), 43.27 KiB | 2.88 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n",
            "total 380\n",
            "drwxr-xr-x 4 root root   4096 Mar 10 01:13 .\n",
            "drwxr-xr-x 1 root root   4096 Mar 10 00:08 ..\n",
            "drwxr-xr-x 8 root root   4096 Mar 10 00:08 .git\n",
            "-rw-r--r-- 1 root root 124125 Mar 10 00:08 hw2-pset.ipynb\n",
            "-rw-r--r-- 1 root root 225936 Mar 10 00:08 My_Checks.ipynb\n",
            "drwxr-xr-x 3 root root   4096 Mar 10 01:13 nlu_hw2\n",
            "-rw-r--r-- 1 root root   4903 Mar 10 00:08 README.md\n",
            "-rw-r--r-- 1 root root   1310 Mar 10 00:08 test_model.py\n",
            "-rw-r--r-- 1 root root   3952 Mar 10 00:08 train_model.py\n",
            "Current directory: /content/nlu_hw2\n",
            "total 380\n",
            "drwxr-xr-x 4 root root   4096 Mar 10 01:13 .\n",
            "drwxr-xr-x 1 root root   4096 Mar 10 00:08 ..\n",
            "drwxr-xr-x 8 root root   4096 Mar 10 00:08 .git\n",
            "-rw-r--r-- 1 root root 124125 Mar 10 00:08 hw2-pset.ipynb\n",
            "-rw-r--r-- 1 root root 225936 Mar 10 00:08 My_Checks.ipynb\n",
            "drwxr-xr-x 3 root root   4096 Mar 10 01:13 nlu_hw2\n",
            "-rw-r--r-- 1 root root   4903 Mar 10 00:08 README.md\n",
            "-rw-r--r-- 1 root root   1310 Mar 10 00:08 test_model.py\n",
            "-rw-r--r-- 1 root root   3952 Mar 10 00:08 train_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets evaluate optuna --quiet # install datasets if it is not included in your environment"
      ],
      "metadata": {
        "id": "8ylNFWYWDsnF"
      },
      "id": "8ylNFWYWDsnF",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections.abc import Iterable\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Model and tokenizer from ðŸ¤— Transformers\n",
        "from transformers import AutoModelForSequenceClassification, \\\n",
        "    BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
        "\n",
        "import torch.nn as nn\n",
        "from typing import Any\n",
        "\n",
        "import evaluate\n",
        "\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "rail9ez6EWLv"
      },
      "id": "rail9ez6EWLv",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am-N-T1NFbL_",
        "outputId": "7a4f8530-5e32-40a9-cc2c-deed1c9470ef"
      },
      "id": "am-N-T1NFbL_",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"prajjwal1/bert-tiny\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLqdDI18Fb_e",
        "outputId": "bf1032a5-b112-4acf-f2e6-cdf9e880f15e"
      },
      "id": "eLqdDI18Fb_e",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code does exactly the same thing as the previous code cell\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"prajjwal1/bert-tiny\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3wemjnlFcJ7",
        "outputId": "96ed9676-1557-497b-ee55-f029053f7510"
      },
      "id": "P3wemjnlFcJ7",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")"
      ],
      "metadata": {
        "id": "TkvicjE_FcTC"
      },
      "id": "TkvicjE_FcTC",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Because ðŸ¤— Transformers supports multiple deep learning libraries, you will\n",
        "# need to use the keyword parameter return_tensors in order to indicate that\n",
        "# you want your inputs to be returned in PyTorch format.\n",
        "inputs = tokenizer([\"Hello world!\", \"How are you?\"], padding=True,\n",
        "                   return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03f0gk7pFccM",
        "outputId": "fd2bbc20-61bb-49fc-9534-ccba42084145"
      },
      "id": "03f0gk7pFccM",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 7592, 2088,  999,  102,    0],\n",
              "        [ 101, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(outputs, end=\"\\n\\n\")\n",
        "\n",
        "# Use the dot operator to access parts of the output\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd_cd1XRFcjr",
        "outputId": "f8f49837-7599-47ca-f1e0-0fc8a98a388b"
      },
      "id": "Qd_cd1XRFcjr",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1629, -0.1532],\n",
            "        [ 0.2339, -0.2121]]), hidden_states=None, attentions=None)\n",
            "\n",
            "tensor([[ 0.1629, -0.1532],\n",
            "        [ 0.2339, -0.2121]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem 1d. Prepare Dataset (Code, 10 Points)**"
      ],
      "metadata": {
        "id": "qQd0ciyMJSDS"
      },
      "id": "qQd0ciyMJSDS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDb dataset and create validation split\n",
        "imdb = load_dataset(\"imdb\")\n",
        "split = imdb[\"train\"].train_test_split(.2, seed=3463)\n",
        "imdb[\"train\"] = split[\"train\"]\n",
        "imdb[\"val\"] = split[\"test\"]\n",
        "del imdb[\"unsupervised\"]"
      ],
      "metadata": {
        "id": "wg7jjTHXFcw0"
      },
      "id": "wg7jjTHXFcw0",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(dataset: Dataset, tokenizer: BertTokenizerFast) \\\n",
        "        -> Dataset:\n",
        "    \"\"\"\n",
        "    Problem 1d: Implement this function.\n",
        "\n",
        "    Preprocesses a dataset using a Hugging Face Tokenizer and prepares\n",
        "    it for use in a Hugging Face Trainer.\n",
        "\n",
        "    :param dataset: A dataset\n",
        "    :param tokenizer: A tokenizer\n",
        "    :return: The dataset, prepreprocessed using the tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        \"\"\"\n",
        "        Tokenizes input text using Hugging Face BERT tokenizer.\n",
        "        Handles multiple edge cases to ensure robust preprocessing.\n",
        "        \"\"\"\n",
        "\n",
        "        # Edge Case: Handle missing or empty text\n",
        "        if \"text\" not in examples or examples[\"text\"] is None:\n",
        "            examples[\"text\"] = [\"[EMPTY]\"] * len(examples.get(\"label\", [0] * len(examples)))\n",
        "\n",
        "        # Edge Case: Handle empty strings or unexpected formats\n",
        "        examples[\"text\"] = [t if isinstance(t, str) and t.strip() else \"[EMPTY]\" for t in examples[\"text\"]]\n",
        "\n",
        "        # Edge Case: Handle different data structures (alternative keys)\n",
        "        if not isinstance(examples[\"text\"], list):\n",
        "            examples[\"text\"] = [str(examples[\"text\"])]\n",
        "\n",
        "        try:\n",
        "            tokenized = tokenizer(\n",
        "                examples[\"text\"],\n",
        "                padding=\"max_length\",  # Ensures all sequences are of equal length\n",
        "                truncation=True,       # Ensures long sequences are truncated\n",
        "                max_length=512        # As per Appendix A.2 of BERT paper\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # Edge Case: Handle tokenization failures\n",
        "            print(f\"Tokenization error: {e}\")\n",
        "            return {\n",
        "                \"input_ids\": [[0] * 512],  # Default empty tokens\n",
        "                \"token_type_ids\": [[0] * 512],\n",
        "                \"attention_mask\": [[0] * 512]\n",
        "            }\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    # Edge Case: Handle large batch sizes by limiting batch processing\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=32)\n",
        "\n",
        "    return tokenized_dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y_xpECDGFcz4"
      },
      "id": "y_xpECDGFcz4",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb[\"train\"] = preprocess_dataset(imdb[\"train\"], tokenizer)\n",
        "imdb[\"val\"] = preprocess_dataset(imdb[\"val\"], tokenizer)\n",
        "imdb[\"test\"] = preprocess_dataset(imdb[\"test\"], tokenizer)\n",
        "\n",
        "# Visualize the preprocessed dataset\n",
        "for k, v in imdb[\"val\"][:2].items():\n",
        "    print(\"{}:\\n{}\\n{}\\n\".format(k, type(v),\n",
        "                                 [item[:20] if isinstance(item, Iterable) else\n",
        "                                 item for item in v[:5]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "a9cdc94ec9ed409dad7fabc6051281fc",
            "420d64a81f804f79ab7e1ed365f0b4df",
            "8033044554fd4516bcc052db09e323db",
            "c996183d354e45a9b37a52eeb4819296",
            "c3b275de197a4cd79a579b3f7555274b",
            "f7b43272fb9f4b798bdeba55d04e9863",
            "5af93445fdb249638831649b72ce2dc6",
            "0c09b02942e3417bb8479b7a0f634d94",
            "057da4b80a074dd68358c1470b22d2bf",
            "64ad2ba9fad84df7adbf6ee0ea0fb877",
            "23224e2591f34376a098fd4692bb8886"
          ]
        },
        "id": "8TSkjHKEFc2h",
        "outputId": "6266a57d-cdf9-4a65-b515-46bc803d6270"
      },
      "id": "8TSkjHKEFc2h",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9cdc94ec9ed409dad7fabc6051281fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:\n",
            "<class 'list'>\n",
            "['As so many others ha', 'When converting a bo']\n",
            "\n",
            "label:\n",
            "<class 'list'>\n",
            "[1, 0]\n",
            "\n",
            "input_ids:\n",
            "<class 'list'>\n",
            "[[101, 2004, 2061, 2116, 2500, 2031, 2517, 1010, 2023, 2003, 1037, 6919, 4516, 1012, 2182, 2003, 1037, 2862, 1997, 1996], [101, 2043, 16401, 1037, 2338, 2000, 2143, 1010, 2009, 2003, 3227, 1037, 2204, 2801, 2000, 2562, 2012, 2560, 2070, 1997]]\n",
            "\n",
            "token_type_ids:\n",
            "<class 'list'>\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "\n",
            "attention_mask:\n",
            "<class 'list'>\n",
            "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 2: Implement Experiment (50 Points in Total)**"
      ],
      "metadata": {
        "id": "pZPKVxdPqxBF"
      },
      "id": "pZPKVxdPqxBF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2a: Freeze Non-Bias Weights (Code, 10 Points)bold text bold text**"
      ],
      "metadata": {
        "id": "UN4YgZ82q41X"
      },
      "id": "UN4YgZ82q41X"
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model(trial: Any, model_name: str, use_bitfit: bool = False) -> \\\n",
        "        BertForSequenceClassification:\n",
        "    \"\"\"\n",
        "    Problem 2a: Implement this function.\n",
        "\n",
        "    This function should be passed to your Trainer's model_init keyword\n",
        "    argument. It will be used by the Trainer to initialize a new model\n",
        "    for each hyperparameter tuning trial. Your implementation of this\n",
        "    function should support training with BitFit by freezing all non-\n",
        "    bias parameters of the initialized model.\n",
        "\n",
        "    :param trial: This parameter is required by the Trainer, but it will\n",
        "        not be used for this problem. Please ignore it\n",
        "    :param model_name: The identifier listed in the Hugging Face Model\n",
        "        Hub for the pre-trained model that will be loaded\n",
        "    :param use_bitfit: If True, then all parameters will be frozen other\n",
        "        than bias terms\n",
        "    :return: A newly initialized pre-trained Transformer classifier\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure default behavior when use_bitfit=False: I added this line\n",
        "    # Becasue during testing my code I got some unexpected output when\n",
        "    # bitfit = False. Adding this line made my output behave correctly.\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True  # Make all parameters trainable by default\n",
        "\n",
        "    # If BitFit is enabled, freeze all non-bias parameters\n",
        "    if use_bitfit:\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"bias\" not in name:  # Freeze all non-bias parameters\n",
        "                param.requires_grad = False\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "etfDB-XOFc49"
      },
      "id": "etfDB-XOFc49",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The first parameter is unused; we just pass None to it\n",
        "model = init_model(None, \"prajjwal1/bert-tiny\", use_bitfit=True)\n",
        "\n",
        "# Check if weight matrix is frozen\n",
        "print(model.bert.encoder.layer[0].attention.self.query.weight.requires_grad)\n",
        "\n",
        "# Check if bias term is frozen\n",
        "print(model.bert.encoder.layer[0].attention.self.query.bias.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmftm5BKFc7e",
        "outputId": "55cb5159-7540-4665-b919-4c722d3871cf"
      },
      "id": "wmftm5BKFc7e",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2b: Set Up Trainer and Tester (Code, 20 Points)**"
      ],
      "metadata": {
        "id": "4bxZZIPun0tA"
      },
      "id": "4bxZZIPun0tA"
    },
    {
      "cell_type": "code",
      "source": [
        "def init_trainer(model_name: str, train_data: Dataset, val_data: Dataset,\n",
        "                 use_bitfit: bool = False) -> Trainer:\n",
        "    \"\"\"\n",
        "    Problem 2b: Implement this function.\n",
        "\n",
        "    Creates a Trainer object that will be used to fine-tune a BERT-tiny\n",
        "    model on the IMDb dataset. The Trainer should fulfill the criteria\n",
        "    listed in the problem set.\n",
        "\n",
        "    :param model_name: The identifier listed in the Hugging Face Model\n",
        "        Hub for the pre-trained model that will be fine-tuned\n",
        "    :param train_data: The training data used to fine-tune the model\n",
        "    :param val_data: The validation data used for hyperparameter tuning\n",
        "    :param use_bitfit: If True, then all parameters will be frozen other\n",
        "        than bias terms\n",
        "    :return: A Trainer used for training\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Load model using init_model function that we developed earlier\n",
        "    model = init_model(None, model_name, use_bitfit)\n",
        "\n",
        "    # Step 2: Define a function for computing accuracy on validation\n",
        "    def compute_metrics(eval_pred):\n",
        "        metric = evaluate.load(\"accuracy\")\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Step 3: Define training arguments for our trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"checkpoints\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=4,              # Train for 4 epochs (as per Problem 1c)\n",
        "        save_total_limit=4,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    # Step 4: Create and return trainer object\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_data,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "KiyY85nPFdAx"
      },
      "id": "KiyY85nPFdAx",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a Trainer from a Hugging Face Model Hub identifier\n",
        "trainer = init_trainer(\"prajjwal1/bert-tiny\", imdb[\"train\"], imdb[\"val\"])\n",
        "\n",
        "# Train using the trainer\n",
        "trainer.train()\n",
        "\n",
        "# Change this to whichever checkpoint you want to evalaute\n",
        "eval_checkpoint_directory = \"checkpoints/run-13/checkpoint-1252\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "GAa0D-xJFdDE",
        "outputId": "c417ff03-1c77-48f0-a7e3-39897163c154"
      },
      "id": "GAa0D-xJFdDE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rboj2qHkFdFm"
      },
      "id": "rboj2qHkFdFm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0IuYIU6yFdIC"
      },
      "id": "0IuYIU6yFdIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYPni9ggFdJ-"
      },
      "id": "UYPni9ggFdJ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCeD_yLGFdNX"
      },
      "id": "GCeD_yLGFdNX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9cdc94ec9ed409dad7fabc6051281fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_420d64a81f804f79ab7e1ed365f0b4df",
              "IPY_MODEL_8033044554fd4516bcc052db09e323db",
              "IPY_MODEL_c996183d354e45a9b37a52eeb4819296"
            ],
            "layout": "IPY_MODEL_c3b275de197a4cd79a579b3f7555274b"
          }
        },
        "420d64a81f804f79ab7e1ed365f0b4df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7b43272fb9f4b798bdeba55d04e9863",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5af93445fdb249638831649b72ce2dc6",
            "value": "Map:â€‡100%"
          }
        },
        "8033044554fd4516bcc052db09e323db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c09b02942e3417bb8479b7a0f634d94",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_057da4b80a074dd68358c1470b22d2bf",
            "value": 5000
          }
        },
        "c996183d354e45a9b37a52eeb4819296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64ad2ba9fad84df7adbf6ee0ea0fb877",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_23224e2591f34376a098fd4692bb8886",
            "value": "â€‡5000/5000â€‡[00:13&lt;00:00,â€‡707.44â€‡examples/s]"
          }
        },
        "c3b275de197a4cd79a579b3f7555274b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b43272fb9f4b798bdeba55d04e9863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5af93445fdb249638831649b72ce2dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c09b02942e3417bb8479b7a0f634d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057da4b80a074dd68358c1470b22d2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64ad2ba9fad84df7adbf6ee0ea0fb877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23224e2591f34376a098fd4692bb8886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}